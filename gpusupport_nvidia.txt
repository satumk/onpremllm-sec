In order to add gpu support, go to the VENV and run this command to change the build of LLAMA to a gpu supported 


CUDAHOSTCXX=/usr/bin/g++-11 CMAKE_ARGS="-DLLAMA_CUBLAS=on" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir


after that, make sure to restart the app, check your gpu usage and check if iy runs out of vram
